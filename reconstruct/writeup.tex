\documentclass[12pt]{article}
\usepackage{fullpage,enumitem,amsmath,amssymb,graphicx}

\begin{document}

\begin{center}
{\Large CS221 Fall 2014 Homework 3}

\begin{tabular}{rl}
SUNet ID: & thomasme \\
Name: & Thomas Mendoza \\
Collaborators: & Steven Samson
\end{tabular}
\end{center}

By turning in this assignment, I agree by the Stanford honor code and declare
that all of this is my own work.

\section*{Problem 1}

\begin{enumerate}[label=(\alph*)]
	\item The greedy algorithm, while it may, in certain circumstances,
		find a solution, falls short in many others. Consider the
		following sentence:

		"the best theatrical performance happened here"

		If the spaces were removed and we had:

		"thebesttheatricalperformancehappenedhere"

		The word "the" would be chosen first since its the smallest cost
		word over the set (nothing else appended to "the" would form a word)
		we would then continue the algorithm from:

		"besttheatricalperformancehappenedhere"

		Here's where the greedy algorithm breaks down. Two potential word
		possibilities arise: "be" and "best". If the shorter word "be" is
		weighted less and therefore chosen by the greedy algorithm
		without further consideration, all words
		chosen after would be neigher fluent nor cost effective.

	\item Programming problem


\end{enumerate}

\section*{Problem 2}

\begin{enumerate}[label=(\alph*)]
	\item Considering the second greedy algorithm, involving vowel insertion,
		the same shortfalls arise as with space insertion. For instance:

		"the scare lifted"

		Removing the vowels we have:
		
		"th scr lftd"

		Again, the word "the" would likely be found since its the lowest-cost,
		full word that can be made first, but when we arrive at "scr" this falls apart.
		Should the shorter, lower-cost bigram("the", "scar") be chosen over
		bigram("the", "scare"), we ensure that future bigrams are now more costly,
		and the sentence loses its fluency.
	
	\item Programming problem

\end{enumerate}

\section*{Problem 3}

\begin{enumerate}[label=(\alph*)]
	\item To formalize this problem as a search problem the model can be
		described as:

		\begin{enumerate}
			\item initial state: beginning of string
			\item goal test: end of string reached
			\item state: previous word and current index
			\item actions: choosing words made from letters between current index and some
				end point
			\item costs: bigram cost between the previous and current word
		\end{enumerate}

		Using the bigram cost function, this is the minimal representation of state
		since we must remember the previous word to compare against new words made
		from the current index.
	
	\item Programming problem

	\item In order to speed up joint space and vowel insertion using A*,
		we can enhance our successor choices with a heuristic. Computing
		another bigram to find out the likelihood of the current guess
		and the next word is expensive, so we need a good approximation
		of the future cost.

		One such approximation would be the unigram cost, \(h_u = u(w)\). This
		is a much cheaper quantity to compute than a bigram. The cost of
		our simpler, faster algorithm can be represented as:
		\[
			\text{cost'} = b(w',w) + u(w)
		\]

		The question is now a matter of consistency. At our goal state,
		which is the end of the string, \(u(w)\) evaluates to \(0\)
		meeting one condition. The next issue to satisfy is the triangle
		inequality, namely that:
		\[
			\text{cost'}(s,a) = \text{cost}(s,a) + h(\text{Succ}(s,a)) - h(s) \geq 0
		\]
		holds true, which is, in our particular case:
		\[
			b(w',w) + u(w) - u(w') \geq 0
		\]
		Barring corner cases, the probablilities of each \(n\)-gram can be represented
		as \(-\text{log}(p(w_n | w_1,...,w_{n-1}))\) which will be substituted into
		our inequality:
		\[
			-\text{log}(p(w | w_1,...,w')) - log(p(w)) + log(p(w')) \geq 0
		\]
		and after moving terms around the inequality, converting the log difference
		to a quotient and exponentiating we have:
		\[
			p(w | w_1,...,w') \leq \frac{p(w')}{p(w)}
		\]
		Some final manipulation yields:
		\[
			 0 \leq p(w') - p(w) \cdot p(w | w_1,...,w')
		\]
		Here we know that \(p(w | w_1,...,w') \leq p(w')\) since the left hand
		quantity depends on \(w'\) and all previous probabilities and that
		even if \(p(w)\) is as large as can be, we get at most identity in the
		previous inequality. Thus since inequality is true for all \(w\)
		and it was created from the consistency requirement, we know that
		our heuristic \(u(w)\) is consistent.


\end{enumerate}

\end{document}
